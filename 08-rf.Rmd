---
title: "08. Random forest"
author: "Al Xin"
date: "1/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Like with preprocessing, the steps will be illustrated on the CDK descriptors from `rcdk` first. Additionally, the process of examining how to tune a random forest model will be performed on a single data set for demonstration. In this case, the example data will be observations for Î±-CD, sourced from the Rekharsky and Inoue data set, with descriptors from `rcdk`.

### General outline for model building

## Dependencies

```{r package}
library(caret)
library(dplyr)
# library(stringr)
library(tibble)
library(tidyr)
library(qsarr)
library(randomForest)
```
```{r}
dir.create("model")
dir.create("model/rf")
dir.create("model/rf/demo")
```

## "Out of the bag" model building

First, the demonstration data set is loaded. 

```{r demo}
# Read preprocessed descriptors
# Remove guest names for training
cdk_list <- read_desc_list(
  list.files("preprocess/cdk", full.names = T), 
  quiet = T
)
# ) %>%
#   retain_name(
#     x = ., 
#     function(x) select(x, -guest)
#   )

# Obtaining the demo data set
demo <- cdk_list[["ri_alpha"]]
```

Without any tuning, the following results are achieved with 10 repetitions of 10-fold cross-validation: 

```{r eval_model}
eval_model <- function(formula, 
                       df, 
                       method, 
                       nfold = 10,
                       resp = 1,
                       simplify = T,
                       ...) {
  fold_index <- createFolds(y = df[, resp], k = nfold)
  
  result <- lapply(
    fold_index, 
    function(ind) {
      trn <- df[-ind, ]
      tst <- df[ind, ]
      param <- list(
        formula = formula, 
        data = trn, 
        ...
      )
      
      model <- switch(
        method, 
        "rf" = do.call(randomForest, param)
      )
      
      pred_y <- predict(model, tst)
      pred_df <- data.frame(tst[resp], pred_y)
      colnames(pred_df) <- c("obs", "pred")
      
      defaultSummary(pred_df)
    }
  ) %>%
    do.call(rbind, .) %>%
    data.frame() %>%
    rownames_to_column(var = "fold") %>%
    pivot_longer(!fold, names_to = "summary_stat")
  
  if (simplify) {
    result %>%
      group_by(summary_stat) %>%
      summarize(
        fold_avg = mean(value), 
        fold_sd = sd(value) #sd_pop
      ) %>%
      data.frame()
  }
  
  result
  # result
}

eval_model_rep <- function(nrep = 10, ...) {
  # print(list(...))
  param <- list(..., simplify = F)
  rep_list <- replicate(
    nrep, 
    do.call(eval_model, param), 
    simplify = F
    )
  
  do.call(rbind, rep_list) %>%
    group_by(summary_stat) %>%
    summarize(
      fold_avg = mean(value), 
      fold_sd = sd(value) #sd_pop
    ) %>%
    data.frame()
}
```

```{r eval_model2}
eval_model <- function(df, 
                       resp = NA,
                       method, 
                       nfold = 10,
                       simplify = T,
                       ignore_col = NA,
                       ...) {
  if (is.null(df[resp])) {
    message("Specify a column name to be used as a response variable")
    return(0)
  }
  
  ignore_index <- which(names(df) %in% ignore_col)
  if(length(ignore_index)) {
    df <- df[, -ignore_index]
  }
  
  fold_index <- createFolds(y = df[, resp], k = nfold)
  
  result <- lapply(
    fold_index, 
    function(ind) {
      trn <- df[-ind, ]
      tst <- df[ind, ]
      param <- list(
        formula = as.formula(paste0(resp, " ~ .")), 
        data = trn, 
        ...
      )
      
      model <- switch(
        method, 
        "rf" = do.call(randomForest, param)
      )
      
      pred_y <- predict(model, tst)
      pred_df <- data.frame(tst[resp], pred_y)
      colnames(pred_df) <- c("obs", "pred")
      
      defaultSummary(pred_df)
    }
  ) %>%
    do.call(rbind, .) %>%
    data.frame() %>%
    rownames_to_column(var = "fold") %>%
    pivot_longer(!fold, names_to = "summary_stat")
  
  if (simplify) {
    result <- result %>%
      group_by(summary_stat) %>%
      summarize(
        fold_avg = mean(value, na.rm = T), 
        fold_sd = sd_pop(value) #sd_pop
      ) %>%
      data.frame()
  }
  
  result
  # result
}

eval_model_rep <- function(nrep = 10, ...) {
  # print(list(...))
  param <- list(..., simplify = F)
  rep_list <- replicate(
    nrep, 
    do.call(eval_model, param), 
    simplify = F
    )
  
  rep_list <- lapply(
    1:length(rep_list), 
    function(x) {
      rep_list[[x]] %>%
        mutate(nrep = x)
    }
  )
  
  df_all <- do.call(rbind, rep_list)
  
  # summary of all folds
  df1 <- df_all %>%
    group_by(summary_stat) %>%
    summarize(
      fold_avg = mean(value, na.rm = T), 
      fold_sd = sd_pop(value), #sd_pop
      .groups = "drop"
    ) %>%
    data.frame()
  
  # summary grouping by replication
  df2 <- df_all %>%
    group_by(summary_stat, nrep) %>%
    summarize(
      x = mean(value, na.rm = T), 
      .groups = "drop"
    ) %>%
    group_by(summary_stat) %>%
    summarize(
      # rep_avg = mean(x), # identical to fold_avg
      rep_sd = sd_pop(x), 
      .groups = "drop"
    ) %>%
    data.frame()
  
  inner_join(df1, df2, by = "summary_stat")
}
```

```{r ootb}
# rf_ctrl <- trainControl(
#   method = "repeatedcv", 
#   number = 10, 
#   repeats = 10
# )
# rf_1 <- train(
#   dG ~ ., 
#   data = df1, 
#   method = "rf", 
#   trControl = rf_ctrl
# )
set.seed(20210105)
demo_ootb <- eval_model_rep(
  nrep = 10, 
  df = demo, 
  method = "rf",  
  resp = "dG", 
  nfold = 10
)

# demo_ootb3 <- eval_model_rep(
#   nrep = 2, 
#   df = demo, 
#   method = "rf",  
#   resp = "dG", 
#   nfold = 3
# )
# 
# demo_rf_ootb2 <- replicate(2, eval_model(
#   # nrep = 10,
#   df = demo,
#   method = "rf",
#   resp = "dG",
#   nfold = 10,
#   simplify = F
# ), simplify = F)

lapply(1:length(demo_rf_ootb2), 
       function(x) {
         demo_rf_ootb2[[x]] %>%
           mutate(nrep = x)
       })
saveRDS(demo_ootb, "model/rf/demo/ootb.RDS")
```
```{r}
demo_ootb <- readRDS("model/rf/demo/ootb.RDS")
knitr::kable(demo_ootb, col.names = c("Measure", "Average", "Std. dev (all)", "Std. dev (repeats)"))
```

## Tuned model

### Using `caret::train`

The package `caret` has an option to tune models using the function `train`. However, for random forests, the tuning only encompasses the parameter `mtry`.

The result reports that the best `mtry` values tested was `mtry = 107`.

```{r}
set.seed(20210106)
demo_ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)
demo_caret_output <- train(
  dG ~ .,
  data = demo,
  method = "rf",
  trControl = demo_ctrl
)
saveRDS(demo_caret_output, "model/rf/demo/caret1.RDS")
```
``` {r}
demo_caret_output <- readRDS("model/rf/demo/caret1.RDS")
print(demo_caret_output)
```

The distribution of the model measures can also be found with `eval_model`.

```{r}
set.seed(20210106)
demo_caret <- eval_model_rep(
  df = demo, 
  method = "rf", 
  nfold = 10,
  nrep = 10,
  resp = "dG", 
  mtry = 107
)
saveRDS(demo_caret, "model/rf/demo/caret2.RDS")
```
```{r}
demo_caret <- readRDS("model/rf/demo/caret2.RDS")
knitr::kable(
  demo_caret, 
  col.names = c("Measure", "Average", "Std. dev. (all)", "Std. dev. (reps)")
)
```

### Using `qsarr::tune`

First, the number of trees

```{r}
tune <- function(method, ...) {
  dummy <- 1
  class(dummy) <- switch(
    method, 
    "rf" = "randomForest"
  )
  UseMethod("tune", dummy)
}

tune_helper <- function(method,
                        df, 
                        resp, 
                        nfold, 
                        nrep, 
                        ignore_col, 
                        ...) {
  # obtaining all combinations of parameters
  tune_grid <- expand.grid(list(...), stringsAsFactors = F)

  tune_grid_list <- split(tune_grid, seq(nrow(tune_grid)))
  names(tune_grid_list) <- NULL

  tune_list <- lapply(
    tune_grid_list, 
    function(x) {
      param <- append(
        x, 
        list(
          df = df, 
          resp = resp,
          method = method,
          nfold = nfold, 
          nrep = nrep, 
          ignore_col = ignore_col
        )
      )
      # print(summary(param))
      do.call(eval_model_rep, param)
    }
  )
  
  # Create list of lists with parameters and results
  names(tune_list) <- NULL
  param_list <- Map(
    function(x, y) {
      list(
        param = x, 
        result = y
      )
    }, 
    x = tune_grid_list, 
    y = tune_list
  )
  
  tune_index <- best_summary_stat(tune_list = tune_list)
  
  list(
    best_param = tune_grid_list[[tune_index]],
    best_result = tune_list[[tune_index]],
    best_index = tune_index,
    all_tune = param_list
  )
  
  # class(obj) <- "tune"
  # obj
}

tune.randomForest <- function(method, 
                              df, 
                              resp, 
                              nfold = 10, 
                              nrep = 1, 
                              ignore_col = NA, 
                              ...) {
  # if (!length(list(...))) {
  #   message("Pass parameters for tuning")
  #   return()
  # }
  # print(method)
  if (missing(resp) || missing(df)) {
    message("Specify a response variable and data frame")
    return()
  }
  
  tune_obj <- tune_helper(
    method = method,
    df = df, 
    resp = resp, 
    nfold = nfold, 
    nrep = nrep, 
    ignore_col = ignore_col, 
    ...
    )
  
  final_param <- append(
    tune_obj$best_param,
    list(
      formula = as.formula(paste0(resp, " ~ .")), 
      data = df
    ), 
    after = 0
  )
  
  final_model <- do.call(randomForest, final_param)
  
  obj <- append(
    tune_obj, 
    list(
      best_model = final_model, 
      param_tested = list(...), 
      nfold_tested = nfold, 
      nrep_tested = nrep
      )
  )
  class(obj) <- "tune"
  obj
}

normalize_summary_stat <- function(tune_list, stat_name) {
  if (!stat_name %in% c("MAE", "Rsquared", "RMSE")) {
    message(
      "Only options for summary statistics are ",
      "MAE, Rsquared, or RMSE")
    return()
  }
  
  raw_val <- sapply(
    tune_list, 
    function(x) {
      x[x$summary_stat == stat_name, "fold_avg"]
    }
  )
  
  val_range <- range(raw_val)
  if (stat_name == "Rsquared") {
    (raw_val - val_range[1])/(val_range[2] - val_range[1])
  } else {
    (val_range[2] - raw_val)/(val_range[2] - val_range[1])
  }
}

best_summary_stat <-
  function(tune_list,
           stat_name_list = c("MAE", "Rsquared", "RMSE")) {
    sum_list <- lapply(stat_name_list,
                       normalize_summary_stat,
                       tune_list = tune_list) %>%
      do.call(rbind, .) %>%
      colSums()
    which.max(sum_list) %>% unlist()
  }

print.tune <- function(tune_obj) {
  cat(
    "Model type: ", class(tune_obj$best_model), "\n\n", 
    "Tuned parameters: ", "\n", 
    sapply(
      names(tune_obj$best_param), 
      function(x) {
        paste0("\t", x, ": ", tune_obj$best_param[[x]], "\n")
      }
    ), "\n",
    "Model performance: ", "\n"
  )
  print(tune_obj$best_result)
}
```
```{r}
set.seed(20210108)
demo_qsarr_ntree <- tune(
  method = "rf", 
  df = demo, 
  resp = "dG", 
  ignore_col = "guest", 
  nfold = 10,
  nrep = 3,
  ntree = c(200, 400, 800, 1100, 1500)
)

saveRDS(demo_qsarr_ntree, "model/rf/demo/qsarr_ntree.RDS")
# No significant improvement in accuracy
# Opting for 300 trees
demo_qsarr <- tune(
  method = "rf", 
  df = demo, 
  resp = "dG", 
  ignore_col = "guest", 
  nfold = 10,
  nrep = 5,
  ntree = 400, 
  replace = c(T, F),
  mtry = c(1, 10, 50, 90), 
  maxnodes = c(NULL, 10, 20, 50)
)
saveRDS(demo_qsarr, "model/rf/demo/qsarr.RDS")
```

```{r}
demo_qsarr <- readRDS("model/rf/demo/qsarr.RDS")
print(demo_qsarr)
# knitr::kable(
#   demo_qsarr$best_result, 
#   col.names = c("Measure", "Average", "Std. deviation")
# )
# cat(
#   "Parameters: ", unlist(demo_qsarr$best_param)
# )
```
Although there is slight improvement across all measures, it is likely not statistically significant considering the standard deviations of the summary statistics. 

## Feature selection

The package `caret` provides a method for filtering features using recursive feature elimination. 

We will run this with the default selections of feature subsets provided by `caret`. 

```{r}
set.seed(20210108)
rfe_ctrl <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 10
)
demo_rfe <- rfe(
  dG ~ ., 
  data = demo, 
  rfeControl = rfe_ctrl, 
  sizes = c(1, 4, 8, 16, 32, 64)
)
saveRDS(demo_rfe, "model/rf/demo/rfe.RDS")
```

Using the defaults, with 10 repetitions of 10-fold cross-validation, we have that 16 variables are selected as providing optimal predictive power. 

These 16 predictors will be used to create `demo2`

```{r}
demo_rfe <- readRDS("model/rf/demo/rfe.RDS")
print(demo_rfe)
rfe_pred <- demo_rfe$optVariables
demo2 <- select(demo, c(dG, rfe_pred))
```

### Tuning

```{r}
set.seed(20210108)
demo2_qsarr <- tune(
  method = "rf", 
  df = demo2, 
  resp = "dG", 
  nfold = 10,
  nrep = 5,
  ntree = 400, 
  replace = c(T, F),
  mtry = c(1, 2, 4, 8, 16), 
  maxnodes = c(NULL, 10, 20, 50)
)
saveRDS(demo2_qsarr, "model/rf/demo/qsarr_rfe.RDS")
```

```{r}
# demo2_qsarr2 <- eval_model(
#   demo2, 
#   resp = "dG", 
#   method = "rf", 
#   nfold = 8,
#   ntree = 400, 
#   mtry = 2, 
#   maxnodes = 50, 
#   simplify = F
# )
demo2_qsarr <- readRDS("model/rf/demo/qsarr_rfe.RDS")
print(demo2_qsarr)
```

With the best tuned model, it is ambiguous whether or not RFE significantly improves or reduces the accuracy of the model building. Given that it makes the model significantly less complicated (reducing more than 100 predictors to 16), it is likely worthwhile to integrate RFE into the model building process. This will make variable importance and interpretation of the model clearer. 

A complete model-building process (from RFE to tuning) can be constructed with the function `build_model`.

```{r}
build_model <- function(model_method, ...) {
  dummy <- 1
  class(dummy) <- switch(
    model_method, 
    "rf" = "randomForest",
    "randomForest" = "randomForest"
  )
  UseMethod("build_model", dummy)
}

build_model.randomForest <- function(df, 
                                     resp, 
                                     nfold = 10, 
                                     nrep = 1, 
                                     ignore_col = NA, 
                                     rfe_ctrl = rfeControl(
                                       functions = rfFuncs, 
                                       method = "repeatedcv", 
                                       repeats = 10
                                     ),
                                     rfe_subset,
                                     ...) {
  ignore_index <- which(names(df) %in% ignore_col)
  if(length(ignore_index)) {
    df <- df[, -ignore_index]
  }
  
  # RFE
  if (missing(rfe_subset)) {
    rfe_subset <- c(2^(0:9))
    rfe_subset <- rfe_subset[rfe_subset < ncol(df)]
  }
  rfe_obj <- rfe(
    as.formula(paste0(resp, " ~ .")),
    data = df,
    rfeControl = rfe_ctrl,
    sizes = rfe_subset
  )

  rfe_pred <- rfe_obj$optVariables
  df2 <- select(df, all_of(c(resp, rfe_pred)))
  
  df2 <- select(df, all_of(c(resp, rfe_pred)))
  
  tune(
    method = "rf", 
    df = df2, 
    resp = "dG", 
    nfold = nfold,
    nrep = nrep,
    ignore_col = ignore_col, 
    ...
    )  
  # print(tune_obj)
  # tune_obj
}

# demo_rf_full <- build_model(
#   model_method = "rf", 
#   df = demo, 
#   resp = "dG", 
#   nfold = 10,
#   nrep = 5,
#   ntree = 300,
#   replace = c(T, F),
#   mtry = c(1, 2, 4, 8, 16),
#   maxnodes = c(NULL, 10, 20, 50)
# )
```

```{r}
set.seed(20210108)
cdk_rf <- lapply(
  cdk_list, 
  build_model, 
  model_method = "rf", 
  df = demo, 
  resp = "dG", 
  nfold = 10,
  nrep = 5,
  ntree = 300,
  replace = c(T, F),
  mtry = c(1, 2, 4, 8, 16),
  maxnodes = c(NULL, 10, 20, 50)
)
dir.create("model/rf/cdk")
cdk_rf_save <- lapply(
  names(cdk_rf), 
  function(x) {
    saveRDS(cdk_rf[[x]], paste0("model/rf/cdk/", x, ".RDS"))
  }
)
# saveRDS(cdk_rf, "model/rf/cdk.RDS")
```

## Other descriptor sources

```{r}
padel_list <- read_desc_list(
  list.files("preprocess/padel", full.names = T), 
  quiet = T
  ) %>%
  retain_name(
    x = .,
    function(x) select(x, -guest)
  )
padel_rf <- lapply(
  padel_list, 
  build_model, 
  model_method = "rf", 
  df = demo, 
  resp = "dG", 
  nfold = 10,
  nrep = 10,
  ntree = 300,
  replace = c(T, F),
  mtry = c(2, 4, 8, 16, 32),
  maxnodes = c(NULL, 10, 20, 50)
)
dir.create("model/rf/padel")
cdk_rf_save <- lapply(
  names(padel_rf), 
  function(x) {
    saveRDS(padel_rf[[x]], paste0("model/rf/padel/", x, ".RDS"))
  }
)
```

```{r}
mordred_list <- read_desc_list(
  list.files("preprocess/mordred", full.names = T), 
  quiet = T
  ) %>%
  retain_name(
    x = .,
    function(x) select(x, -guest)
  )
mordred_rf <- lapply(
  mordred_list, 
  build_model, 
  model_method = "rf", 
  df = demo, 
  resp = "dG", 
  nfold = 10,
  nrep = 10,
  ntree = 300,
  replace = c(T, F),
  mtry = c(2, 4, 8, 16, 32),
  maxnodes = c(NULL, 10, 20, 50)
)
dir.create("model/rf/mordred")
cdk_rf_save <- lapply(
  names(mordred_rf), 
  function(x) {
    saveRDS(mordred_rf[[x]], paste0("model/rf/mordred/", x, ".RDS"))
  }
)
```

```{r}
ochem_list <- read_desc_list(
  list.files("preprocess/ochem", full.names = T), 
  quiet = T
  ) %>%
  retain_name(
    x = .,
    function(x) select(x, -guest)
  )
ochem_rf <- lapply(
  ochem_list, 
  build_model, 
  model_method = "rf", 
  df = demo, 
  resp = "dG", 
  nfold = 10,
  nrep = 10,
  ntree = 300,
  replace = c(T, F),
  mtry = c(2, 4, 8, 16, 32),
  maxnodes = c(NULL, 10, 20, 50)
)
dir.create("model/rf/ochem")
cdk_rf_save <- lapply(
  names(ochem_rf), 
  function(x) {
    saveRDS(ochem_rf[[x]], paste0("model/rf/ochem/", x, ".RDS"))
  }
)
```

## External validation

```{r}
sd_pop <- function (x, quiet = T) {
  if (sum(is.na(x))) {
      if (!quiet) 
          message("Removing NAs when finding std. deviation")
      x <- x[!is.na(x)]
  }
  if (sum(is.nan(x))) {
      if (!quiet) 
          message("Removing NaNs when finding std. deviation")
      x <- x[!is.nan(x)]
  }
  sd <- sum((x - mean(x))^2)
  sqrt(sd/length(x))
}
```

