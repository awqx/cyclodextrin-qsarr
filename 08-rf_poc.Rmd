---
title: "08. Random forest proof of concept"
author: "Al Xin"
date: "1/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Like with preprocessing, the steps will be illustrated on the CDK descriptors from `rcdk` first. Additionally, the process of examining how to tune a random forest model will be performed on a single data set for demonstration. In this case, the example data will be observations for Î±-CD, sourced from the Rekharsky and Inoue data set, with descriptors from `rcdk`.

### General outline for model building

## Dependencies

```{r package}
library(caret)
library(dplyr)
# library(stringr)
library(tibble)
library(tidyr)
library(qsarr)
library(randomForest)
```
```{r}
if (!dir.exists("model")) dir.create("model")
if (!dir.exists("model/rf_poc")) dir.create("model/rf_poc")
if (!dir.exists("model/rf_poc/demo")) {
  dir_save <- sapply(
    paste0("model/rf_poc/"), 
    c("demo", "cdk", "mordred", "ochem", "padel")
  )  
}

```

## "Out of the bag" model building

First, the demonstration data set is loaded. 

```{r demo}
# Read preprocessed descriptors
# Remove guest names for training
cdk_list <- read_desc_list(
  list.files("preprocess/cdk", full.names = T), 
  quiet = T
)
# ) %>%
#   retain_name(
#     x = ., 
#     function(x) select(x, -guest)
#   )

# Obtaining the demo data set
demo <- cdk_list[["ri_alpha"]]
```

Without any tuning, the following results are achieved with 10 repetitions of 10-fold cross-validation: 

```{r eval_model}
# eval_model <- function(formula, 
#                        df, 
#                        method, 
#                        nfold = 10,
#                        resp = 1,
#                        simplify = T,
#                        ...) {
#   fold_index <- createFolds(y = df[, resp], k = nfold)
#   
#   result <- lapply(
#     fold_index, 
#     function(ind) {
#       trn <- df[-ind, ]
#       tst <- df[ind, ]
#       param <- list(
#         formula = formula, 
#         data = trn, 
#         ...
#       )
#       
#       model <- switch(
#         method, 
#         "rf" = do.call(randomForest, param)
#       )
#       
#       pred_y <- predict(model, tst)
#       pred_df <- data.frame(tst[resp], pred_y)
#       colnames(pred_df) <- c("obs", "pred")
#       
#       defaultSummary(pred_df)
#     }
#   ) %>%
#     do.call(rbind, .) %>%
#     data.frame() %>%
#     rownames_to_column(var = "fold") %>%
#     pivot_longer(!fold, names_to = "summary_stat")
#   
#   if (simplify) {
#     result %>%
#       group_by(summary_stat) %>%
#       summarize(
#         fold_avg = mean(value), 
#         fold_sd = sd(value) #sd_pop
#       ) %>%
#       data.frame()
#   }
#   
#   result
#   # result
# }
# 
# eval_model_rep <- function(nrep = 10, ...) {
#   # print(list(...))
#   param <- list(..., simplify = F)
#   rep_list <- replicate(
#     nrep, 
#     do.call(eval_model, param), 
#     simplify = F
#     )
#   
#   do.call(rbind, rep_list) %>%
#     group_by(summary_stat) %>%
#     summarize(
#       fold_avg = mean(value), 
#       fold_sd = sd_pop(value) 
#     ) %>%
#     data.frame()
# }
```

```{r eval_model2}
eval_model <- function(df, 
                       resp = NA,
                       method, 
                       nfold = 10,
                       simplify = T,
                       ignore_col = NA,
                       ...) {
  if (is.null(df[resp])) {
    message("Specify a column name to be used as a response variable")
    return(0)
  }
  
  ignore_index <- which(names(df) %in% ignore_col)
  if (length(ignore_index)) {
    df <- df[, -ignore_index]
  }
  
  fold_index <- createFolds(y = df[, resp], k = nfold)
  
  result <- lapply(
    fold_index, 
    function(ind) {
      trn <- df[-ind, ]
      tst <- df[ind, ]
      param <- list(
        formula = as.formula(paste0(resp, " ~ .")), 
        data = trn, 
        ...
      )
      
      model <- switch(
        method, 
        "rf" = do.call(randomForest, param)
      )
      
      pred_y <- predict(model, tst)
      pred_df <- data.frame(tst[resp], pred_y)
      colnames(pred_df) <- c("obs", "pred")
      
      defaultSummary(pred_df)
    }
  ) %>%
    do.call(rbind, .) %>%
    data.frame() %>%
    rownames_to_column(var = "fold") %>%
    pivot_longer(!fold, names_to = "summary_stat")
  
  if (simplify) {
    result <- result %>%
      group_by(summary_stat) %>%
      summarize(
        fold_avg = mean(value, na.rm = T), 
        fold_sd = sd_pop(value) #sd_pop
      ) %>%
      data.frame()
  }
  
  result
  # result
}

eval_model_rep <- function(nrep = 10, ...) {
  # print(list(...))
  param <- list(..., simplify = F)
  rep_list <- replicate(
    nrep, 
    do.call(eval_model, param), 
    simplify = F
    )
  
  rep_list <- lapply(
    1:length(rep_list), 
    function(x) {
      rep_list[[x]] %>%
        mutate(nrep = x)
    }
  )
  
  df_all <- do.call(rbind, rep_list)
  
  # summary of all folds
  df1 <- df_all %>%
    group_by(summary_stat) %>%
    summarize(
      fold_avg = mean(value, na.rm = T), 
      fold_sd = sd_pop(value), #sd_pop
      .groups = "drop"
    ) %>%
    data.frame()
  
  # summary grouping by replication
  df2 <- df_all %>%
    group_by(summary_stat, nrep) %>%
    summarize(
      x = mean(value, na.rm = T), 
      .groups = "drop"
    ) %>%
    group_by(summary_stat) %>%
    summarize(
      # rep_avg = mean(x), # identical to fold_avg
      rep_sd = sd_pop(x), 
      .groups = "drop"
    ) %>%
    data.frame()
  
  
  df_summary <- inner_join(df1, df2, by = "summary_stat")
  
  # Not collapsing the folds
  df3 <- df_all %>%
    group_by(summary_stat, nrep) %>%
    summarize(
      avg = mean(value, na.rm = T), 
      sd = sd_pop(value), 
      .groups = "drop"
    ) %>%
    data.frame()
  
  list(result_summary = df_summary, result_all = df3)
}
```

```{r ootb}
# rf_ctrl <- trainControl(
#   method = "repeatedcv", 
#   number = 10, 
#   repeats = 10
# )
# rf_1 <- train(
#   dG ~ ., 
#   data = df1, 
#   method = "rf", 
#   trControl = rf_ctrl
# )
set.seed(20210105)
demo_ootb <- eval_model_rep(
  nrep = 10, 
  df = demo, 
  method = "rf",  
  resp = "dG", 
  nfold = 10, 
  ignore_col = "guest"
)

# demo_ootb3 <- eval_model_rep(
#   nrep = 2, 
#   df = demo, 
#   method = "rf",  
#   resp = "dG", 
#   nfold = 3
# )
# 
# demo_rf_ootb2 <- replicate(2, eval_model(
#   # nrep = 10,
#   df = demo,
#   method = "rf",
#   resp = "dG",
#   nfold = 10,
#   simplify = F
# ), simplify = F)

# lapply(1:length(demo_rf_ootb2), 
#        function(x) {
#          demo_rf_ootb2[[x]] %>%
#            mutate(nrep = x)
#        })

saveRDS(demo_ootb, "model/rf_poc/demo/ootb.RDS")
```
```{r}
demo_ootb <- readRDS("model/rf_poc/demo/ootb.RDS")
knitr::kable(demo_ootb, col.names = c("Measure", "Average", "Std. dev (all)", "Std. dev (repeats)"))
```

## Tuned model

### Using `caret::train`

The package `caret` has an option to tune models using the function `train`. However, for random forests, the tuning only encompasses the parameter `mtry`.

The result reports that the best `mtry` values tested was `mtry = 107`.

```{r}
set.seed(20210106)
demo_ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)
demo_caret_output <- train(
  dG ~ .,
  data = demo,
  method = "rf",
  trControl = demo_ctrl
)
saveRDS(demo_caret_output, "model/rf_poc/demo/caret1.RDS")
```
``` {r}
demo_caret_output <- readRDS("model/rf_poc/demo/caret1.RDS")
print(demo_caret_output)
```

The distribution of the model measures can also be found with `eval_model`.

```{r}
set.seed(20210106)
demo_caret <- eval_model_rep(
  df = demo, 
  method = "rf", 
  nfold = 10,
  nrep = 10,
  resp = "dG", 
  mtry = 107, 
  ignore_col = "guest"
)
saveRDS(demo_caret, "model/rf_poc/demo/caret2.RDS")
```
```{r}
demo_caret <- readRDS("model/rf_poc/demo/caret2.RDS")
knitr::kable(
  demo_caret, 
  col.names = c("Measure", "Average", "Std. dev. (all)", "Std. dev. (reps)")
)
```

### Using `qsarr::tune`

First, the number of trees

```{r}
tune <- function(method, ...) {
  if (missing(method)) {
    messages("Sp")
    return()
  }
  dummy <- 1
  class(dummy) <- switch(
    method, 
    "rf" = "randomForest"
  )
  UseMethod("tune", dummy)
}

tune_helper <- function(method,
                        df, 
                        resp, 
                        nfold, 
                        nrep, 
                        ignore_col, 
                        ...) {
  # obtaining all combinations of parameters
  tune_grid <- expand.grid(list(...), stringsAsFactors = F)
  
  tune_grid_list <- split(tune_grid, seq(nrow(tune_grid)))
  names(tune_grid_list) <- NULL

  tune_list <- lapply(
    tune_grid_list, 
    function(x) {
      param <- append(
        x, 
        list(
          df = df, 
          resp = resp,
          method = method,
          nfold = nfold, 
          nrep = nrep, 
          ignore_col = ignore_col
        )
      )
      # print(summary(param))
      do.call(eval_model_rep, param)
    }
  )
  
  # Create list of lists with parameters and results
  names(tune_list) <- NULL
  param_list <- Map(
    function(x, y) {
      list(
        param = x, 
        result = y$result_summary
      )
    }, 
    x = tune_grid_list, 
    y = tune_list
  )
  
  tune_index <- best_summary_stat(tune_list = tune_list)
  
  list(
    param = tune_grid_list[[tune_index]],
    result_summary = tune_list[[tune_index]]$result_summary,
    result_all = tune_list[[tune_index]]$result_all,
    best_index = tune_index,
    all_tune = param_list
  )
}

tune.randomForest <- function(method, 
                              df, 
                              resp, 
                              nfold = 10, 
                              nrep = 1, 
                              ignore_col = NA,
                              ...) {
  if (missing(resp) || missing(df)) {
    message("Specify a response variable and data frame")
    return()
  }

  tune_obj <- tune_helper(
    method = method,
    df = df, 
    resp = resp, 
    nfold = nfold, 
    nrep = nrep, 
    ignore_col = ignore_col, 
    ...
    )
  
  if (!is.na(ignore_col)) {
    ignore_index <- which(names(df) %in% ignore_col)
    if (length(ignore_index)) {
      df <- df[,-ignore_index]
    }
  }
  
  final_param <- append(
    tune_obj$param,
    list(
      formula = as.formula(paste0(resp, " ~ .")), 
      data = df
    ), 
    after = 0
  )
  
  final_model <- do.call(randomForest, final_param)
  
  obj <- append(
    tune_obj, 
    list(
      model = final_model, 
      param_tested = list(...), 
      nfold_tested = nfold, 
      nrep_tested = nrep, 
      pred_name = names(df)[names(df) != resp]
      )
  )
  class(obj) <- "tune"
  obj
}

normalize_summary_stat <- function(tune_list, stat_name) {
  if (!stat_name %in% c("MAE", "Rsquared", "RMSE")) {
    message(
      "Only options for summary statistics are ",
      "MAE, Rsquared, or RMSE")
    return()
  }
  
  raw_val <- sapply(
    tune_list, 
    function(x) {
      x <- x$result_summary
      x[x$summary_stat == stat_name, "fold_avg"]
    }
  )
  
  val_range <- range(raw_val, na.rm = T)
  if (stat_name == "Rsquared") {
    (raw_val - val_range[1])/(val_range[2] - val_range[1])
  } else {
    (val_range[2] - raw_val)/(val_range[2] - val_range[1])
  }
}

best_summary_stat <-
  function(tune_list,
           stat_name_list = c("MAE", "Rsquared", "RMSE")) {
    sum_list <- lapply(stat_name_list,
                       normalize_summary_stat,
                       tune_list = tune_list) %>%
      do.call(rbind, .) %>%
      colSums()
    result <- which.max(sum_list) %>% unlist()
    if (!length(result)) return(1)
    result
  }

print.tune <- function(tune_obj) {
  cat(
    "Model type: ", class(tune_obj$model), "\n\n", 
    "Tuned parameters: ", "\n", 
    sapply(
      names(tune_obj$param), 
      function(x) {
        paste0("\t", x, ": ", tune_obj$param[[x]], "\n")
      }
    ), "\n",
    "Number of predictors: ", length(tune_obj$pred_name), "\n", 
    "Predictors (first 10): ", 
    tune_obj$pred_name[1:(min(10, length(tune_obj$pred_name)))], 
    "\n", "\n",
    "Model performance: ", "\n"
  )
  print(tune_obj$result_summary)
}
```
```{r}
set.seed(20210108)
demo_qsarr_ntree <- tune(
  method = "rf", 
  df = demo, 
  resp = "dG", 
  nfold = 10,
  nrep = 10,
  ignore_col = "guest",
  ntree = c(200, 400, 800, 1100, 1500)
)

saveRDS(demo_qsarr_ntree, "model/rf_poc/demo/qsarr_ntree.RDS")
# No significant improvement in accuracy
# Opting for 400 trees
demo_qsarr <- tune(
  method = "rf", 
  df = demo, 
  resp = "dG", 
  ignore_col = "guest", 
  nfold = 10,
  nrep = 5,
  ntree = 400, 
  replace = c(T, F),
  mtry = c(1, 10, 50, 90), 
  maxnodes = c(NULL, 10, 20, 50)
)
saveRDS(demo_qsarr, "model/rf_poc/demo/qsarr.RDS")
```

```{r}
demo_qsarr <- readRDS("model/rf_poc/demo/qsarr.RDS")
print(demo_qsarr)
# knitr::kable(
#   demo_qsarr$best_result, 
#   col.names = c("Measure", "Average", "Std. deviation")
# )
# cat(
#   "Parameters: ", unlist(demo_qsarr$best_param)
# )
```
Although there is slight improvement across all measures, it is likely not statistically significant considering the standard deviations of the summary statistics. 

## Feature selection

The package `caret` provides a method for filtering features using recursive feature elimination. 

We will run this with the default selections of feature subsets provided by `caret`. 

```{r}
set.seed(20210108)
rfe_ctrl <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 10
)
demo_rfe <- rfe(
  dG ~ . - guest, 
  data = demo, 
  rfeControl = rfe_ctrl, 
  sizes = c(1, 4, 8, 16, 32, 64)
)
saveRDS(demo_rfe, "model/rf_poc/demo/rfe.RDS")
```

Using the defaults, with 10 repetitions of 10-fold cross-validation, we have that 16 variables are selected as providing optimal predictive power. 

These 16 predictors will be used to create `demo2`

```{r}
demo_rfe <- readRDS("model/rf_poc/demo/rfe.RDS")
print(demo_rfe)
rfe_pred <- demo_rfe$optVariables
demo2 <- select(demo, c(dG, rfe_pred))
```

### Tuning

```{r}
set.seed(20210108)
demo2_qsarr <- tune(
  method = "rf", 
  df = demo2, 
  resp = "dG", 
  nfold = 10,
  nrep = 5,
  ntree = 400, 
  replace = c(T, F),
  mtry = c(1, 2, 4, 8, 16), 
  maxnodes = c(NULL, 10, 20, 50)
)
saveRDS(demo2_qsarr, "model/rf_poc/demo/qsarr_rfe.RDS")
```

```{r}
# demo2_qsarr2 <- eval_model(
#   demo2, 
#   resp = "dG", 
#   method = "rf", 
#   nfold = 8,
#   ntree = 400, 
#   mtry = 2, 
#   maxnodes = 50, 
#   simplify = F
# )
demo2_qsarr <- readRDS("model/rf_poc/demo/qsarr_rfe.RDS")
print(demo2_qsarr)
```

With the best tuned model, it is ambiguous whether or not RFE significantly improves or reduces the accuracy of the model building. Given that it makes the model significantly less complicated (reducing more than 100 predictors to 16), it is likely worthwhile to integrate RFE into the model building process. This will make variable importance and interpretation of the model clearer. 

A complete model-building process (from RFE to tuning) can be constructed with the function `build_model`.

```{r}
build_model <- function(model_method, ...) {
  dummy <- 1
  class(dummy) <- switch(
    model_method, 
    "rf" = "randomForest",
    "randomForest" = "randomForest"
  )
  UseMethod("build_model", dummy)
}

build_model.randomForest <- function(df, 
                                     resp, 
                                     nfold = 10, 
                                     nrep = 1, 
                                     ignore_col = NA, 
                                     rfe_ctrl,
                                     rfe_subset,
                                     ...) {
  if (!is.na(ignore_col[1])) {
    ignore_index <- which(names(df) %in% ignore_col)
    if(length(ignore_index)) {
      df <- df[, -ignore_index]
    }
  }
  
  
  # RFE
  if (missing(rfe_subset)) {
    rfe_subset <- c(2^(0:6))
    rfe_subset <- rfe_subset[rfe_subset < ncol(df)]
  }
  
  # Create a generic rfeControl object if not provided
  if (missing(rfe_ctrl)) {
    rfe_ctrl <- rfeControl(
      functions = rfFuncs,
      method = "repeatedcv",
      repeats = 10)
  }
  
  rfe_obj <- rfe(
    as.formula(paste0(resp, " ~ .")),
    data = df,
    rfeControl = rfe_ctrl,
    sizes = rfe_subset
  )

  rfe_pred <- rfe_obj$optVariables
  df2 <- select(df, all_of(c(resp, rfe_pred)))

  tune(
    method = "rf", 
    df = df2, 
    resp = "dG", 
    nfold = nfold,
    nrep = nrep,
    ignore_col = ignore_col, 
    ...
    )  
  # print(tune_obj)
  # tune_obj
}

# demo_rf_full <- build_model(
#   model_method = "rf", 
#   df = demo, 
#   resp = "dG", 
#   nfold = 10,
#   nrep = 5,
#   ntree = 300,
#   replace = c(T, F),
#   mtry = c(1, 2, 4, 8, 16),
#   maxnodes = c(NULL, 10, 20, 50)
# )
```

```{r}
set.seed(20210108)
cdk_rf <- lapply(
  cdk_list, 
  build_model, 
  model_method = "rf", 
  resp = "dG", 
  ignore_col = "guest",
  nfold = 5,
  nrep = 2,
  ntree = 400,
  replace = c(T, F),
  mtry = c(1, 2, 4, 8)
)

cdk_rf_save <- lapply(
  names(cdk_rf), 
  function(x) {
    saveRDS(cdk_rf[[x]], paste0("model/rf_poc/cdk/", x, ".RDS"))
  }
)

```

## Other descriptor sources

For the other descriptors, the columns `guest` was removed prior to analysis. However, this can also be accomplished using `ignore_col = "guest"`, as demonstrated in the analysis using the CDK models above.

```{r}
padel_list <- read_desc_list(
  list.files("preprocess/padel", full.names = T), 
  quiet = T
  ) %>%
  retain_name(
    x = .,
    function(x) select(x, -guest)
  )
padel_rf <- lapply(
  padel_list, 
  build_model, 
  model_method = "rf", 
  resp = "dG", 
  nfold = 10,
  nrep = 10,
  ntree = 300,
  replace = c(T, F)
  # mtry = c(1, 2, 4, 8, 12)
)

padel_rf_save <- lapply(
  names(padel_rf), 
  function(x) {
    saveRDS(padel_rf[[x]], paste0("model/rf_poc/padel/", x, ".RDS"))
  }
)

set.seed(20210111)
mordred_list <- read_desc_list(
  list.files("preprocess/mordred", full.names = T), 
  quiet = T
  ) %>%
  retain_name(
    x = .,
    function(x) select(x, -guest)
  )
mordred_rf <- lapply(
  mordred_list, 
  build_model, 
  model_method = "rf", 
  resp = "dG", 
  nfold = 10,
  nrep = 10,
  ntree = 300,
  replace = c(T, F),
  mtry = c(1, 2, 4)
)

mordred_rf_save <- lapply(
  names(mordred_rf), 
  function(x) {
    saveRDS(mordred_rf[[x]], paste0("model/rf_poc/mordred/", x, ".RDS"))
  }
)

set.seed(20210108)
ochem_list <- read_desc_list(
  list.files("preprocess/ochem", full.names = T), 
  quiet = T
  ) %>%
  retain_name(
    x = .,
    function(x) select(x, -guest)
  )
ochem_rf <- lapply(
  ochem_list, 
  build_model, 
  model_method = "rf", 
  resp = "dG", 
  nfold = 10,
  nrep = 10,
  # different rfe_ctrl due to number of descriptors
  rfe_ctrl = rfeControl(
    functions = rfFuncs,
    method = "repeatedcv",
    repeats = 5, 
    number = 5
  ),
  ntree = 400,
  replace = c(T, F),
  mtry = c(1, 2, 4, 8, 12)
)

ochem_rf_save <- lapply(
  names(ochem_rf), 
  function(x) {
    saveRDS(ochem_rf[[x]], paste0("model/rf_poc/ochem/", x, ".RDS"))
  }
)
```

## External validation

```{r}
sd_pop <- function (x, quiet = T) {
  if (sum(is.na(x))) {
      if (!quiet) 
          message("Removing NAs when finding std. deviation")
      x <- x[!is.na(x)]
  }
  if (sum(is.nan(x))) {
      if (!quiet) 
          message("Removing NaNs when finding std. deviation")
      x <- x[!is.nan(x)]
  }
  sd <- sum((x - mean(x))^2)
  sqrt(sd/length(x))
}
```

